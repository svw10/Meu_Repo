name: pesquisando-web
description: O Pesquisador de Intelig√™ncia Competitiva. Usa Apify para buscas Google, scraping de sites e extra√ß√£o estruturada. Especializado em an√°lise de concorr√™ncia (SaaS, e-commerce, tend√™ncias de mercado).
version: 3.0.0
tags: [apify, scraping, crawler, research, competitive-intelligence, market-analysis]
---

# Pesquisando Web (Intelligence Protocol v3.0)

## üéØ Quando usar
- **An√°lise Competitiva**: "Como funciona o try-on da Zeekit?", "Pre√ßos e features do Vue.ai".
- **D√∫vidas Factuais**: "Qual o pre√ßo atual do iPhone 15?", "Quem √© o CEO da empresa X?".
- **Monitoramento Cont√≠nuo**: "Verifique se saiu vaga nova no LinkedIn da Zenvia", "Mudan√ßas de pre√ßo do concorrente".
- **Enriquecimento de Dados**: Usu√°rio forneceu URL, extrair resumo estruturado.
- **Tend√™ncias de Mercado**: "Micro-SaaS trends 2026", "State of AI Fashion Tech".

> **Foco Especial**: An√°lise de concorr√™ncia em **fashion tech**, **virtual try-on** e **e-commerce SaaS**.

---

## üß± Stack de Intelig√™ncia

| Componente | Ferramenta | Fun√ß√£o | Custo Aprox. |
| :--- | :--- | :--- | :--- |
| **Busca** | Apify Google Search Scraper | Resultados SERP, rich snippets | $0.001/query |
| **Scraping Leve** | Apify Cheerio Scraper | HTML est√°tico, blogs, docs | $0.002/page |
| **Scraping Pesado** | Apify Puppeteer Scraper | SPAs, JavaScript-heavy | $0.01/page |
| **An√°lise** | OpenRouter (GPT-4o/Claude) | Sumariza√ß√£o, extra√ß√£o estruturada | Vari√°vel |
| **Orquestra√ß√£o** | Inngest | Filas, retries, agendamento | - |
| **Storage** | Neon | Cache de resultados, hist√≥rico | - |

### Actors do Apify Recomendados

| Actor ID | Uso | Quando Usar |
| :--- | :--- | :--- |
| `apify/google-search-scraper` | Busca Google | Dados gerais, SERP, posicionamento |
| `apify/website-content-crawler` | Crawl completo | Mapear site inteiro de concorrente |
| `apify/cheerio-scraper` | P√°gina √∫nica | Extrair pricing, features, docs |
| `apify/puppeteer-scraper` | SPA/JS-heavy | Dashboards, apps modernos |
| `apify/linkedin-profile-scraper` | Pessoas | Hiring, key people (compliance!) |
| `apify/twitter-scraper` | Social | Sentimento, an√∫ncios |

---

## ‚öôÔ∏è Fluxo de Trabalho

### Passo 1: Defini√ß√£o da Miss√£o de Pesquisa (2 min)

**Classifica√ß√£o da Pesquisa:**

```typescript
type ResearchType = 
  | 'competitor-analysis'    // An√°lise completa de concorrente
  | 'pricing-intelligence'   // Monitoramento de pre√ßos
  | 'feature-comparison'     // Comparativo de funcionalidades
  | 'market-trends'          // Tend√™ncias de mercado
  | 'factual-query'          // Resposta direta
  | 'monitoring';            // Verifica√ß√£o peri√≥dica
```

**Template de Briefing:**

```markdown
## Miss√£o de Pesquisa: [Nome do Concorrente/Tema]

**Objetivo**: [O que precisamos saber?]
**Alvo**: [URL espec√≠fica ou termo de busca]
**Profundidade**: 
  - [ ] Superficial (t√≠tulos + snippets)
  - [ ] M√©dia (conte√∫do principal das p√°ginas)
  - [ ] Profunda (m√∫ltiplas p√°ginas, an√°lise cruzada)

**Entreg√°veis Esperados**:
- [ ] Resumo executivo (3 par√°grafos)
- [ ] Dados estruturados (JSON: pricing, features, etc.)
- [ ] Screenshots/evid√™ncias (URLs visitadas)
- [ ] An√°lise de gaps (o que eles t√™m que n√£o temos)

**Restri√ß√µes**:
- [ ] N√£o violar ToS (rate limiting, robots.txt)
- [ ] N√£o scrapear √°reas logadas
- [ ] Respeitar cache (n√£o re-scrapear < 24h)
```

---

### Passo 2: Execu√ß√£o Estrat√©gica (5 min)

**A. Busca Google (Descoberta)**

```typescript
// Para an√°lise de concorr√™ncia: encontre todos os players
const searchQueries = [
  "Zeekit virtual try on pricing",
  "Vue.ai alternatives competitors",
  "AI fashion try on SaaS 2024",
  "virtual fitting room API pricing",
  "SnapFit vs Zeekit vs Vue.ai"
];
```

**B. Scraping Direcionado (Extra√ß√£o)**

| Alvo | Actor | Dados Alvo |
| :--- | :--- | :--- |
| `/pricing` | Cheerio | Planos, limites, features por tier |
| `/features` | Cheerio | Lista de funcionalidades, screenshots |
| `/docs/api` | Cheerio | Documenta√ß√£o t√©cnica, endpoints |
| `/about` | Cheerio | Funding, team size, investors |
| Blog/Case studies | Crawler | Messaging, positioning, ICP |

**C. An√°lise Competitiva Estruturada**

```typescript
interface CompetitorAnalysis {
  company: {
    name: string;
    website: string;
    founded?: string;
    funding?: string;
    employees?: string;
  };
  product: {
    category: string;
    positioning: string;  // "Enterprise" vs "SMB"
    keyDifferentiators: string[];
    techStack?: string[]; // Inferido de docs, job postings
  };
  pricing: {
    model: 'usage-based' | 'seat-based' | 'hybrid';
    tiers: Array<{
      name: string;
      price: string;
      limits: Record<string, string>;
      features: string[];
    }>;
    freeTier: boolean;
    enterpriseContact: boolean;
  };
  strengths: string[];
  weaknesses: string[];
  opportunities: string[]; // Para n√≥s: gaps que podemos explorar
  threats: string[];       // Riscos para nossa posi√ß√£o
}
```

---

### Passo 3: Processamento e S√≠ntese (5 min)

**Pipeline de An√°lise:**

1. **Limpeza**: Remover HTML, scripts, navbars (usando readability ou LLM)
2. **Extra√ß√£o Estruturada**: Converter texto em JSON tipado
3. **Compara√ß√£o**: Cruzar com nossos dados (se dispon√≠vel)
4. **Sumariza√ß√£o**: Gerar insights acion√°veis

```typescript
// Exemplo: Extra√ß√£o de pricing via LLM
const pricingPrompt = `
Analise o texto abaixo de uma p√°gina de pre√ßos e extraia:
1. Nome de cada plano
2. Pre√ßo (mensal/anual)
3. Limites principais (requests, usu√°rios, etc.)
4. Features inclu√≠das em cada tier
5. Se h√° free trial ou free tier

Texto: """${scrapedContent}"""

Responda em JSON v√°lido seguindo o schema CompetitorPricing.
`;
```

---

### Passo 4: Entrega e Arquivamento (2 min)

**Formatos de Sa√≠da:**

```markdown
# Relat√≥rio de Intelig√™ncia Competitiva: [Concorrente]

## Executive Summary
[3-5 par√°grafos com insights chave]

## Ficha T√©cnica
| Atributo | Valor |
| :--- | :--- |
| Website | [URL] |
| Funding | [Dado] |
| Pricing | [Link / Resumo] |

## An√°lise de Pricing
[Tabela comparativa]

## Feature Matrix
[Checklist de funcionalidades]

## Oportunidades Identificadas
1. [Gap que podemos explorar]
2. [Diferencia√ß√£o poss√≠vel]

## Evid√™ncias
- [URLs visitadas com timestamps]
- [Screenshots relevantes]
- [Trechos de c√≥digo/docs]
```

**Arquivamento:**
- [ ] Salvar no banco (competitor_intelligence table)
- [ ] Taggear com data, tipo de pesquisa, solicitante
- [ ] Alertar se mudan√ßas significativas detectadas (vs. cache)

---

## üìã Checklist de Qualidade e √âtica

### Qualidade dos Dados
- [ ] M√∫ltiplas fontes cruzadas (n√£o confiar em uma √∫nica p√°gina)?
- [ ] Dados com data de coleta (intelig√™ncia envelhece r√°pido)?
- [ ] Screenshots/evid√™ncias para claims importantes?
- [ ] Cache configurado (evitar re-scraping desnecess√°rio)?

### √âtica e Compliance
- [ ] Respeitando robots.txt?
- [ ] Rate limiting adequado (n√£o sobrecarregar servidores)?
- [ ] N√£o acessando √°reas logadas/protegidas?
- [ ] GDPR/CCPA: n√£o coletando PII desnecess√°ria?
- [ ] Termos de Uso do site alvo permitem scraping?

### Custo
- [ ] Actor mais barato suficiente para o job (Cheerio > Puppeteer)?
- [ ] Limitando maxResults/maxPages?
- [ ] Reutilizando resultados em cache quando poss√≠vel?

---

## üíª Snippets de Produ√ß√£o

### apify_client.ts (Cliente Tipado Completo)

```typescript
// lib/apify/client.ts
import { ApifyClient } from 'apify-client';

const apify = new ApifyClient({
  token: process.env.APIFY_API_TOKEN,
});

// Interfaces de resultado
export interface GoogleSearchResult {
  title: string;
  url: string;
  description: string;
  position: number;
}

export interface ScrapedContent {
  url: string;
  title: string;
  text: string;
  html: string;
  metadata: Record<string, string>;
}

export interface CompetitorData {
  pricing?: any;
  features?: string[];
  description?: string;
}

export class WebResearcher {
  // Busca Google com cache
  async searchGoogle(
    query: string, 
    options: {
      maxResults?: number;
      includeAds?: boolean;
      languageCode?: string;
    } = {}
  ): Promise<GoogleSearchResult[]> {
    const { maxResults = 5, includeAds = false, languageCode = 'pt' } = options;

    const run = await apify.actor("apify/google-search-scraper").call({
      queries: query,
      resultsPerPage: maxResults,
      maxPagesPerQuery: 1,
      languageCode,
      includeUnfilteredResults: false,
      includeAds,
    });

    const { items } = await apify.dataset(run.defaultDatasetId).listItems();
    
    return items.map((item: any) => ({
      title: item.title,
      url: item.url,
      description: item.description,
      position: item.position,
    }));
  }

  // Scraping inteligente (escolhe actor baseado na p√°gina)
  async scrapePage(url: string, options: {
    waitForSelector?: string;
    javascriptEnabled?: boolean;
  } = {}): Promise<ScrapedContent> {
    const actorId = options.javascriptEnabled 
      ? "apify/puppeteer-scraper" 
      : "apify/cheerio-scraper";

    const run = await apify.actor(actorId).call({
      startUrls: [{ url }],
      maxRequestsPerCrawl: 1,
      ...(options.waitForSelector && { waitForSelector: options.waitForSelector }),
    });

    const { items } = await apify.dataset(run.defaultDatasetId).listItems();
    const item = items[0];

    return {
      url: item.url,
      title: item.title,
      text: item.text || item.content,
      html: item.html,
      metadata: {
        loadTime: item.loadTime,
        pageTitle: item.pageTitle,
      },
    };
  }

  // Crawl completo de site (para an√°lise profunda)
  async crawlSite(startUrl: string, options: {
    maxPages?: number;
    includeUrls?: string[];
    excludeUrls?: string[];
  } = {}): Promise<ScrapedContent[]> {
    const { maxPages = 10 } = options;

    const run = await apify.actor("apify/website-content-crawler").call({
      startUrls: [{ url: startUrl }],
      maxCrawlPages: maxPages,
      crawlerType: "cheerio", // ou "playwright" para JS-heavy
    });

    const results: ScrapedContent[] = [];
    for await (const item of apify.dataset(run.defaultDatasetId).iterateItems()) {
      results.push({
        url: item.url,
        title: item.title,
        text: item.text,
        html: item.html,
        metadata: item.metadata,
      });
    }

    return results;
  }

  // An√°lise de concorrente espec√≠fica
  async analyzeCompetitor(domain: string): Promise<CompetitorData> {
    // 1. P√°gina principal
    const home = await this.scrapePage(`https://${domain}`);
    
    // 2. P√°gina de pricing (tentativa)
    let pricing = null;
    try {
      const pricingPage = await this.scrapePage(`https://${domain}/pricing`);
      pricing = await this.extractPricing(pricingPage.text);
    } catch (e) {
      console.log("Pricing page not found or accessible");
    }

    return {
      description: home.text.slice(0, 500),
      pricing,
    };
  }

  private async extractPricing(text: string): Promise<any> {
    // Usar LLM para extrair estrutura de pricing
    // Implementa√ß√£o depende do cliente OpenRouter
    return null;
  }
}

export const researcher = new WebResearcher();
```

### research_workflow.ts (Workflow Inngest Completo)

```typescript
// inngest/functions/research.ts
import { inngest } from "@/inngest/client";
import { researcher } from "@/lib/apify/client";
import { openrouter } from "@/lib/openrouter/client"; // Assumindo cliente similar
import { db } from "@/lib/db";

interface ResearchEvent {
  query: string;
  type: 'competitor-analysis' | 'pricing' | 'trends' | 'factual';
  depth: 'shallow' | 'medium' | 'deep';
  competitorDomain?: string;
  saveToDb?: boolean;
}

export const researchWorkflow = inngest.createFunction(
  {
    id: "competitive-intelligence",
    name: "Competitive Intelligence Research",
    concurrency: 3, // Evitar rate limits
    retries: 2,
  },
  { event: "app/research.start" },
  async ({ event, step, logger }) => {
    const { query, type, depth, competitorDomain, saveToDb = true } = event.data as ResearchEvent;

    logger.info(`Iniciando pesquisa: ${type} - ${query}`);

    // 1. Busca inicial (se n√£o for an√°lise direta de dom√≠nio)
    let searchResults = [];
    if (!competitorDomain) {
      searchResults = await step.run("google-search", async () => {
        return await researcher.searchGoogle(query, {
          maxResults: depth === 'deep' ? 10 : 5,
        });
      });
    }

    // 2. Scraping de p√°ginas relevantes
    const scrapedData = await step.run("scraping", async () => {
      const urlsToScrape = competitorDomain 
        ? [`https://${competitorDomain}`]
        : searchResults.slice(0, depth === 'deep' ? 5 : 3).map(r => r.url);

      const results = await Promise.all(
        urlsToScrape.map(url => 
          researcher.scrapePage(url).catch(err => ({ error: err.message, url }))
        )
      );

      return results.filter(r => !('error' in r));
    });

    // 3. An√°lise com LLM
    const analysis = await step.run("llm-analysis", async () => {
      const context = scrapedData.map(d => `
URL: ${d.url}
Title: ${d.title}
Content: ${d.text.slice(0, 3000)}
---`).join('\n');

      const prompt = type === 'competitor-analysis' 
        ? generateCompetitorPrompt(query, context)
        : generateGeneralPrompt(query, context);

      const response = await openrouter.generate([
        { role: 'system', content: 'Voc√™ √© um analista de intelig√™ncia competitiva.' },
        { role: 'user', content: prompt },
      ]);

      return response.content;
    });

    // 4. Parse estruturado (tentativa)
    let structuredData = null;
    try {
      structuredData = JSON.parse(analysis);
    } catch {
      structuredData = { summary: analysis, error: "N√£o foi poss√≠vel parsear JSON" };
    }

    // 5. Persistir no banco
    if (saveToDb) {
      await step.run("persist-results", async () => {
        await db.insert(researchLogs).values({
          query,
          type,
          depth,
          results: searchResults,
          analysis: structuredData,
          scrapedUrls: scrapedData.map(d => d.url),
          createdAt: new Date(),
        });
      });
    }

    return {
      success: true,
      query,
      type,
      summary: typeof structuredData === 'object' ? structuredData.summary : analysis,
      data: structuredData,
      sources: scrapedData.map(d => ({ url: d.url, title: d.title })),
    };
  }
);

function generateCompetitorPrompt(query: string, context: string): string {
  return `
Analise o concorrente baseado nos dados coletados e forne√ßa:

1. Resumo executivo (2-3 par√°grafos)
2. JSON estruturado com:
   - company: { name, positioning, targetAudience }
   - pricing: { model, tiers[], freeTier }
   - product: { keyFeatures[], differentiators[], techStack[] }
   - strengths: string[]
   - weaknesses: string[]
   - opportunities: string[] (gaps para explorarmos)

Dados coletados:
${context}

Query original: ${query}

Responda em portugu√™s, JSON v√°lido seguido de an√°lise textual.
`;
}

function generateGeneralPrompt(query: string, context: string): string {
  return `
Responda √† pergunta "${query}" baseado nos dados coletados.

Dados:
${context}

Forne√ßa:
1. Resposta direta e factual
2. Fontes consultadas
3. N√≠vel de confian√ßa (alto/m√©dio/baixo)
4. Recomenda√ß√µes de pr√≥ximos passos se a informa√ß√£o for insuficiente
`;
}
```

### test_search.py (Script de Teste)

```python
#!/usr/bin/env python3
"""
Script de teste r√°pido para validar integra√ß√£o Apify.
√ötil para verificar credenciais antes de deploy.
"""

import os
import sys
import json
from apify_client import ApifyClient

def check_env():
    """Verifica vari√°veis necess√°rias."""
    token = os.getenv("APIFY_API_TOKEN")
    if not token:
        print("‚ùå APIFY_API_TOKEN n√£o encontrado")
        print("   Defina: export APIFY_API_TOKEN='seu_token_aqui'")
        sys.exit(1)
    return token

def test_google_search(client, query="fashion tech startups 2024"):
    """Testa busca Google."""
    print(f"\nüîé Testando Google Search: '{query}'")
    
    try:
        run = client.actor("apify/google-search-scraper").call({
            "queries": query,
            "maxPagesPerQuery": 1,
            "resultsPerPage": 3,
        })
        
        results = []
        for item in client.dataset(run["defaultDatasetId"]).iterate_items():
            results.append({
                "title": item.get("title"),
                "url": item.get("url"),
                "description": item.get("description")[:100] + "..."
            })
        
        print(f"‚úÖ Sucesso! {len(results)} resultados encontrados")
        for r in results:
            print(f"   ‚Ä¢ {r['title'][:50]}...")
        return True
        
    except Exception as e:
        print(f"‚ùå Falha: {e}")
        return False

def test_scraping(client, url="https://example.com"):
    """Testa scraping simples."""
    print(f"\nüåê Testando Scraping: {url}")
    
    try:
        run = client.actor("apify/cheerio-scraper").call({
            "startUrls": [{"url": url}],
            "maxRequestsPerCrawl": 1,
        })
        
        items = list(client.dataset(run["defaultDatasetId"]).iterate_items())
        if items:
            item = items[0]
            print(f"‚úÖ Sucesso! T√≠tulo: {item.get('title')}")
            print(f"   Texto extra√≠do: {len(item.get('text', ''))} caracteres")
            return True
        else:
            print("‚ö†Ô∏è Nenhum dado retornado")
            return False
            
    except Exception as e:
        print(f"‚ùå Falha: {e}")
        return False

def main():
    print("üß™ Teste de Integra√ß√£o Apify")
    print("=" * 50)
    
    token = check_env()
    client = ApifyClient(token=token)
    
    # Testes
    tests = [
        ("Google Search", lambda: test_google_search(client)),
        ("Web Scraping", lambda: test_scraping(client)),
    ]
    
    results = []
    for name, test_fn in tests:
        try:
            success = test_fn()
            results.append((name, success))
        except Exception as e:
            print(f"‚ùå {name} erro inesperado: {e}")
            results.append((name, False))
    
    # Resumo
    print("\n" + "=" * 50)
    print("üìä Resumo dos Testes:")
    for name, success in results:
        status = "‚úÖ PASSOU" if success else "‚ùå FALHOU"
        print(f"   {name}: {status}")
    
    if all(r[1] for r in results):
        print("\nüéâ Todos os testes passaram! Apify configurado corretamente.")
        sys.exit(0)
    else:
        print("\n‚ö†Ô∏è Alguns testes falharam. Verifique configura√ß√£o.")
        sys.exit(1)

if __name__ == "__main__":
    main()
```

---

## üóÇÔ∏è Estrutura de Arquivos

```
Minhas Skills/pesquisando-web/
‚îú‚îÄ‚îÄ SKILL.md                              # Este protocolo
‚îú‚îÄ‚îÄ snippets/
‚îÇ   ‚îú‚îÄ‚îÄ apify_client.ts                   # Cliente tipado completo
‚îÇ   ‚îú‚îÄ‚îÄ research_workflow.ts              # Workflow Inngest
‚îÇ   ‚îî‚îÄ‚îÄ competitive_analysis_template.ts  # Templates de an√°lise
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ test_search.py                    # Teste de integra√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ competitor_monitor.py             # Monitoramento cont√≠nuo
‚îÇ   ‚îî‚îÄ‚îÄ extract_structured.py             # Extra√ß√£o via LLM
‚îî‚îÄ‚îÄ resources/
    ‚îú‚îÄ‚îÄ briefing_template.md              # Template de miss√£o
    ‚îú‚îÄ‚îÄ output_template.md                # Template de relat√≥rio
    ‚îî‚îÄ‚îÄ competitor_schema.json            # Schema de dados estruturados
```

